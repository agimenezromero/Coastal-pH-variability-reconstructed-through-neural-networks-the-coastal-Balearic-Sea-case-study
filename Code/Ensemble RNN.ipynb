{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from datetime import datetime\n",
    "from time import time\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import SimpleRNN\n",
    "from keras.layers import GRU\n",
    "\n",
    "from tensorflow.keras.callbacks import Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN_dataset(data, n_times, n_features):\n",
    "    \n",
    "    X = np.zeros((len(data)-n_times, n_times, n_features))\n",
    "    Y = np.zeros(len(data)-n_times)\n",
    "\n",
    "    for i in range(len(data) - n_times):\n",
    "\n",
    "        X[i] = data[i:n_times+i, 0:n_features]\n",
    "        Y[i] = data[n_times+i, -1]\n",
    "        \n",
    "    return X, Y\n",
    "\n",
    "def RNN_dataset_pred(data, n_times, n_features):\n",
    "    \n",
    "    X = np.zeros((len(data)-n_times, n_times, n_features))\n",
    "\n",
    "    for i in range(len(data) - n_times):\n",
    "\n",
    "        X[i] = data[i:n_times+i, 0:n_features]\n",
    "        \n",
    "    return X\n",
    "    \n",
    "def preprocessing(data, n_times=24, test_size=0.2):\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    scaled = scaler.fit_transform(data)\n",
    "\n",
    "    data_f = scaled\n",
    "    \n",
    "    n_features = data.shape[-1] - 1\n",
    "\n",
    "    X, Y = RNN_dataset(data_f, n_times, n_features)\n",
    "    \n",
    "    idxs = []\n",
    "\n",
    "    for i in range(len(X)):\n",
    "\n",
    "        if str(Y[i]) == 'nan':\n",
    "\n",
    "            idxs.append(i)\n",
    "\n",
    "        else:\n",
    "\n",
    "            j = 0\n",
    "\n",
    "            for item in X[i]:\n",
    "\n",
    "                if str(item[0]) == 'nan' or str(item[1]) == 'nan':\n",
    "\n",
    "                    #print(\"nan found\")\n",
    "                    idxs.append(i)\n",
    "\n",
    "                    break\n",
    "\n",
    "                j+= 1\n",
    "\n",
    "        i += 1\n",
    "        \n",
    "    \n",
    "    X_new = np.zeros((X.shape[0] - len(idxs), X.shape[1], X.shape[2]))\n",
    "    Y_new = np.zeros(len(Y) - len(idxs))\n",
    "\n",
    "    k = 0\n",
    "\n",
    "    for i in range(len(X)):\n",
    "\n",
    "        if i not in idxs:\n",
    "\n",
    "            X_new[k] = X[i]\n",
    "            Y_new[k] = Y[i]\n",
    "\n",
    "            k += 1\n",
    "\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X_new, Y_new, test_size=test_size, random_state=4)\n",
    "    \n",
    "    print(\"Training set:\", X_train.shape, \"Test set:\", X_test.shape)\n",
    "    \n",
    "    return X_train, X_test, Y_train, Y_test, scaler, X_new, Y_new\n",
    "\n",
    "class PrintCrossPoint(Callback):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.epoch_cross = \"\"\n",
    "        self.epoch = 0\n",
    "     \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        \n",
    "        self.epoch += 1\n",
    "        \n",
    "        logs = logs or {}\n",
    "        \n",
    "        current_train_loss = logs.get(\"loss\")\n",
    "        current_val_loss = logs.get(\"val_loss\")\n",
    "        \n",
    "        if current_val_loss < current_train_loss:\n",
    "            \n",
    "            if self.epoch_cross == \"\":\n",
    "                self.epoch_cross = self.epoch\n",
    "                \n",
    "            #self.model.stop_training = True\n",
    "            \n",
    "    def on_train_end(self, epoch, logs=None):\n",
    "        \n",
    "        print(\"Validation loss higher than training loss from epoch %s!\" % self.epoch_cross)\n",
    "        \n",
    "class StopCrossPoint(Callback):\n",
    "    \n",
    "    def __init__(self):\n",
    "    \n",
    "        self.epoch = 0\n",
    "     \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        \n",
    "        self.epoch += 1\n",
    "        \n",
    "        logs = logs or {}\n",
    "        \n",
    "        current_train_loss = logs.get(\"loss\")\n",
    "        current_val_loss = logs.get(\"val_loss\")\n",
    "        \n",
    "        if current_val_loss < current_train_loss:\n",
    "                \n",
    "            #print(\"Validation loss higher than training loss from epoch %s!\" % self.epoch)\n",
    "                \n",
    "            self.model.stop_training = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 6\n",
    "\n",
    "\n",
    "#df_f = pd.read_csv(\"Datos pH Baleares/Palma_resampled_dayly.csv\")\n",
    "df_f = pd.read_csv(\"Datos pH Baleares/Corrected_dataset_Palma.csv\")\n",
    "\n",
    "df_f[\"Time\"] = pd.to_datetime(df_f[\"Time\"])\n",
    "\n",
    "#Delete red points\n",
    "df_final = df_f.drop(df_f[(df_f[\"Time\"] > datetime(2019, 8, 15)) & (df_f[\"Time\"] < datetime(2019, 9, 1))].index)\n",
    "\n",
    "df_final = df_final.drop(df_final[(df_final[\"Time\"] > datetime(2020, 6, 15)) & (df_final[\"Time\"] < datetime(2020, 7, 1))].index)\n",
    "\n",
    "#data_resampled = df_final[df_final[\"DO(umol kg-1)\"].astype('str') != 'nan'][[\"Tempertaure (ºC)\", \"DO(umol kg-1)\", \"pHT\"]].values\n",
    "data_resampled = df_final[df_final[\"Oxygen\"].astype('str') != 'nan'][[\"Temperature\", \"Oxygen\", \"PH\"]].values\n",
    "\n",
    "X_train, X_test, Y_train, Y_test, scaler, X_scaled, Y_scaled = preprocessing(data_resampled, n_times=window_size, test_size=0.1)\n",
    "\n",
    "#data_new = df_final[[\"Tempertaure (ºC)\", \"DO(umol kg-1)\"]].values\n",
    "data_new = df_final[[\"Temperature\", \"Oxygen\"]].values\n",
    "\n",
    "scaled_new = scaler.min_[0:data_new.shape[-1]] + data_new * scaler.scale_[0:data_new.shape[-1]]\n",
    "\n",
    "n_features = data_new.shape[-1]\n",
    "\n",
    "X_to_predict = RNN_dataset_pred(scaled_new, window_size, n_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN(Nf=100, tol=0.8):\n",
    "\n",
    "    final_train_loss_RNN = 1000.0\n",
    "\n",
    "    train_errors = []\n",
    "    val_errors = []\n",
    "    N_epochs = []\n",
    "    training_time = []\n",
    "\n",
    "    slopes = []\n",
    "    intercepts = []\n",
    "\n",
    "    N = 0\n",
    "    tol = 0.8\n",
    "\n",
    "    while N < Nf:\n",
    "\n",
    "        callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=2, min_delta=0.00001)\n",
    "        callback_2 = StopCrossPoint()\n",
    "\n",
    "        t0 = time()\n",
    "\n",
    "        # design network\n",
    "        model_RNN = Sequential()\n",
    "        model_RNN.add(SimpleRNN(3, activation='tanh', input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "        model_RNN.add(Dense(1, activation='sigmoid'))\n",
    "        model_RNN.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "        # fit network\n",
    "        history_RNN = model_RNN.fit(\n",
    "            X_train, \n",
    "            Y_train, \n",
    "            epochs=500,\n",
    "            steps_per_epoch=10,\n",
    "            #batch_size=100, \n",
    "            validation_data=(X_test, Y_test), \n",
    "            verbose=0, \n",
    "            shuffle=False,\n",
    "            callbacks=[callback, callback_2]\n",
    "            )\n",
    "\n",
    "        time_elapsed_RNN =  time()-t0\n",
    "        epochs_used_RNN = len(history_RNN.history['loss'])\n",
    "\n",
    "        final_train_loss_RNN = (history_RNN.history['loss'][-1]*100)\n",
    "        final_val_loss_RNN = (history_RNN.history['val_loss'][-1]*100)\n",
    "\n",
    "        init_train_loss_RNN = (history_RNN.history['loss'][0]*100)\n",
    "\n",
    "        y_pred_RNN = model_RNN.predict(X_to_predict)\n",
    "\n",
    "        y_pred_noscale_RNN = (y_pred_RNN - scaler.min_[-1]) / scaler.scale_[-1]\n",
    "\n",
    "        yhat_RNN = model_RNN.predict(X_scaled)\n",
    "\n",
    "        y_pred_noscale_RNN[:, 0][df_final[\"PH\"][window_size:].astype('str') != 'nan'] = df_final[\"PH\"][window_size:][df_final[\"PH\"][window_size:].astype('str') != 'nan'].values\n",
    "\n",
    "        time_delta = df_final[\"Time\"][window_size:] - df_final[\"Time\"][window_size]\n",
    "\n",
    "        time_years = np.array([(item / np.timedelta64(1, 'm')) / (60*24*365) for item in time_delta])\n",
    "\n",
    "        Y_TREND = y_pred_noscale_RNN[y_pred_noscale_RNN.astype('str') != 'nan']\n",
    "        X_TREND = time_years[y_pred_noscale_RNN[:, 0].astype('str') != 'nan']\n",
    "\n",
    "        reg = LinearRegression().fit(X_TREND.reshape(-1,1), Y_TREND)\n",
    "\n",
    "        slope_RNN = reg.coef_[0]\n",
    "        intercept_RNN = reg.intercept_\n",
    "\n",
    "        if final_train_loss_RNN < tol:\n",
    "\n",
    "            print(N)\n",
    "\n",
    "            N += 1\n",
    "\n",
    "            slopes.append(slope_RNN)\n",
    "            intercepts.append(intercept_RNN)\n",
    "\n",
    "            train_errors.append(final_train_loss_RNN)\n",
    "            val_errors.append(final_val_loss_RNN)\n",
    "\n",
    "            N_epochs.append(epochs_used_RNN)\n",
    "            training_time.append(time_elapsed_RNN)\n",
    "            \n",
    "    return slopes, intercepts, train_errors, val_errors, N_epochs, training_time\n",
    "\n",
    "def LSTM_NN(Nf=100, tol=0.8):\n",
    "\n",
    "    final_train_loss_LSTM = 1000.0\n",
    "\n",
    "    train_errors = []\n",
    "    val_errors = []\n",
    "    N_epochs = []\n",
    "    training_time = []\n",
    "\n",
    "    slopes = []\n",
    "    intercepts = []\n",
    "\n",
    "    N = 0\n",
    "    tol = 0.8\n",
    "\n",
    "    while N < Nf:\n",
    "\n",
    "        callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=2, min_delta=0.00001)\n",
    "        callback_2 = StopCrossPoint()     \n",
    "\n",
    "        t0 = time()\n",
    "\n",
    "        # design network\n",
    "        model_LSTM = Sequential()\n",
    "        model_LSTM.add(LSTM(3, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "        model_LSTM.add(Dense(1, activation='sigmoid'))\n",
    "        model_LSTM.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "        # fit network\n",
    "        history_LSTM = model_LSTM.fit(\n",
    "            X_train, \n",
    "            Y_train, \n",
    "            epochs=500,\n",
    "            steps_per_epoch=10,\n",
    "            #batch_size=100, \n",
    "            validation_data=(X_test, Y_test), \n",
    "            verbose=0, \n",
    "            shuffle=False,\n",
    "            callbacks=[callback, callback_2]\n",
    "            )\n",
    "\n",
    "        time_elapsed_LSTM =  time()-t0\n",
    "        epochs_used_LSTM = len(history_LSTM.history['loss'])\n",
    "\n",
    "        final_train_loss_LSTM = (history_LSTM.history['loss'][-1]*100)\n",
    "        final_val_loss_LSTM = (history_LSTM.history['val_loss'][-1]*100)\n",
    "\n",
    "        init_train_loss_LSTM = (history_LSTM.history['loss'][0]*100)\n",
    "\n",
    "        y_pred_LSTM = model_LSTM.predict(X_to_predict)\n",
    "\n",
    "        y_pred_noscale_LSTM = (y_pred_LSTM - scaler.min_[-1]) / scaler.scale_[-1]\n",
    "\n",
    "        yhat_LSTM = model_LSTM.predict(X_scaled)\n",
    "        y_pred_noscale_LSTM[:, 0][df_final[\"PH\"][window_size:].astype('str') != 'nan'] = df_final[\"PH\"][window_size:][df_final[\"PH\"][window_size:].astype('str') != 'nan'].values\n",
    "\n",
    "        time_delta = df_final[\"Time\"][window_size:] - df_final[\"Time\"][window_size]\n",
    "\n",
    "        time_years = np.array([(item / np.timedelta64(1, 'm')) / (60*24*365) for item in time_delta])\n",
    "\n",
    "        Y_TREND = y_pred_noscale_LSTM[y_pred_noscale_LSTM.astype('str') != 'nan']\n",
    "        X_TREND = time_years[y_pred_noscale_LSTM[:, 0].astype('str') != 'nan']\n",
    "\n",
    "        reg = LinearRegression().fit(X_TREND.reshape(-1,1), Y_TREND)\n",
    "\n",
    "        slope_LSTM = reg.coef_[0]\n",
    "        intercept_LSTM = reg.intercept_\n",
    "\n",
    "        if final_train_loss_LSTM < tol:\n",
    "\n",
    "            print(N)\n",
    "\n",
    "            N += 1\n",
    "\n",
    "            slopes.append(slope_LSTM)\n",
    "            intercepts.append(intercept_LSTM)\n",
    "\n",
    "            train_errors.append(final_train_loss_LSTM)\n",
    "            val_errors.append(final_val_loss_LSTM)\n",
    "\n",
    "            N_epochs.append(epochs_used_LSTM)\n",
    "            training_time.append(time_elapsed_LSTM)\n",
    "            \n",
    "    return slopes, intercepts, train_errors, val_errors, N_epochs, training_time\n",
    "\n",
    "def BI_LSTM_NN(Nf=100, tol=0.8):\n",
    "\n",
    "    final_train_loss_BI_LSTM = 1000.0\n",
    "\n",
    "    train_errors = []\n",
    "    val_errors = []\n",
    "    N_epochs = []\n",
    "    training_time = []\n",
    "\n",
    "    slopes = []\n",
    "    intercepts = []\n",
    "\n",
    "    N = 0\n",
    "    tol = 0.8\n",
    "\n",
    "    while N < Nf:\n",
    "\n",
    "        callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=2, min_delta=0.00001)\n",
    "        callback_2 = StopCrossPoint()     \n",
    "\n",
    "        t0 = time()\n",
    "\n",
    "        # design network\n",
    "        model_BI_LSTM = Sequential()\n",
    "        model_BI_LSTM.add(Bidirectional(LSTM(3, activation='tanh',\n",
    "                                      input_shape=(X_train.shape[1], X_train.shape[2]))))\n",
    "        model_BI_LSTM.add(Dense(1, activation='sigmoid'))\n",
    "        model_BI_LSTM.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "        # fit network\n",
    "        history_BI_LSTM = model_BI_LSTM.fit(\n",
    "            X_train, \n",
    "            Y_train, \n",
    "            epochs=500,\n",
    "            steps_per_epoch=10,\n",
    "            #batch_size=100, \n",
    "            validation_data=(X_test, Y_test), \n",
    "            verbose=0, \n",
    "            shuffle=False,\n",
    "            callbacks=[callback, callback_2]\n",
    "            )\n",
    "\n",
    "        time_elapsed_BI_LSTM =  time()-t0\n",
    "        epochs_used_BI_LSTM = len(history_BI_LSTM.history['loss'])\n",
    "\n",
    "        final_train_loss_BI_LSTM = (history_BI_LSTM.history['loss'][-1]*100)\n",
    "        final_val_loss_BI_LSTM = (history_BI_LSTM.history['val_loss'][-1]*100)\n",
    "\n",
    "        init_train_loss_BI_LSTM = (history_BI_LSTM.history['loss'][0]*100)\n",
    "\n",
    "        y_pred_BI_LSTM = model_BI_LSTM.predict(X_to_predict)\n",
    "\n",
    "        y_pred_noscale_BI_LSTM = (y_pred_BI_LSTM - scaler.min_[-1]) / scaler.scale_[-1]\n",
    "\n",
    "        yhat_BI_LSTM = model_BI_LSTM.predict(X_scaled)\n",
    "        y_pred_noscale_BI_LSTM[:, 0][df_final[\"PH\"][window_size:].astype('str') != 'nan'] = df_final[\"PH\"][window_size:][df_final[\"PH\"][window_size:].astype('str') != 'nan'].values\n",
    "\n",
    "        time_delta = df_final[\"Time\"][window_size:] - df_final[\"Time\"][window_size]\n",
    "\n",
    "        time_years = np.array([(item / np.timedelta64(1, 'm')) / (60*24*365) for item in time_delta])\n",
    "\n",
    "        Y_TREND = y_pred_noscale_BI_LSTM[y_pred_noscale_BI_LSTM.astype('str') != 'nan']\n",
    "        X_TREND = time_years[y_pred_noscale_BI_LSTM[:, 0].astype('str') != 'nan']\n",
    "\n",
    "        reg = LinearRegression().fit(X_TREND.reshape(-1,1), Y_TREND)\n",
    "\n",
    "        slope_BI_LSTM = reg.coef_[0]\n",
    "        intercept_BI_LSTM = reg.intercept_\n",
    "\n",
    "        if final_train_loss_BI_LSTM < tol:\n",
    "\n",
    "            print(N)\n",
    "\n",
    "            N += 1\n",
    "\n",
    "            slopes.append(slope_BI_LSTM)\n",
    "            intercepts.append(intercept_BI_LSTM)\n",
    "\n",
    "            train_errors.append(final_train_loss_BI_LSTM)\n",
    "            val_errors.append(final_val_loss_BI_LSTM)\n",
    "\n",
    "            N_epochs.append(epochs_used_BI_LSTM)\n",
    "            training_time.append(time_elapsed_BI_LSTM)\n",
    "            \n",
    "    return slopes, intercepts, train_errors, val_errors, N_epochs, training_time\n",
    "\n",
    "def BI_GRU_NN(Nf=100, tol=0.8):\n",
    "\n",
    "    final_train_loss_BI_GRU = 1000.0\n",
    "\n",
    "    train_errors = []\n",
    "    val_errors = []\n",
    "    N_epochs = []\n",
    "    training_time = []\n",
    "\n",
    "    slopes = []\n",
    "    intercepts = []\n",
    "\n",
    "    N = 0\n",
    "    tol = 0.8\n",
    "\n",
    "    while N < Nf:\n",
    "\n",
    "        callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=2, min_delta=0.00001)\n",
    "        callback_2 = StopCrossPoint()     \n",
    "\n",
    "        t0 = time()\n",
    "\n",
    "        # design network\n",
    "        model_BI_GRU = Sequential()\n",
    "        model_BI_GRU.add(Bidirectional(GRU(1, activation='tanh',\n",
    "                                      input_shape=(X_train.shape[1], X_train.shape[2]))))\n",
    "        model_BI_GRU.add(Dense(1, activation='sigmoid'))\n",
    "        model_BI_GRU.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "        # fit network\n",
    "        history_BI_GRU = model_BI_GRU.fit(\n",
    "            X_train, \n",
    "            Y_train, \n",
    "            epochs=500,\n",
    "            steps_per_epoch=10,\n",
    "            #batch_size=100, \n",
    "            validation_data=(X_test, Y_test), \n",
    "            verbose=0, \n",
    "            shuffle=False,\n",
    "            callbacks=[callback, callback_2]\n",
    "            )\n",
    "\n",
    "        time_elapsed_BI_GRU =  time()-t0\n",
    "        epochs_used_BI_GRU = len(history_BI_GRU.history['loss'])\n",
    "\n",
    "        final_train_loss_BI_GRU = (history_BI_GRU.history['loss'][-1]*100)\n",
    "        final_val_loss_BI_GRU = (history_BI_GRU.history['val_loss'][-1]*100)\n",
    "\n",
    "        init_train_loss_BI_GRU = (history_BI_GRU.history['loss'][0]*100)\n",
    "\n",
    "        y_pred_BI_GRU = model_BI_GRU.predict(X_to_predict)\n",
    "\n",
    "        y_pred_noscale_BI_GRU = (y_pred_BI_GRU - scaler.min_[-1]) / scaler.scale_[-1]\n",
    "\n",
    "        yhat_BI_GRU = model_BI_GRU.predict(X_scaled)\n",
    "        y_pred_noscale_BI_GRU[:, 0][df_final[\"PH\"][window_size:].astype('str') != 'nan'] = df_final[\"PH\"][window_size:][df_final[\"PH\"][window_size:].astype('str') != 'nan'].values\n",
    "\n",
    "        time_delta = df_final[\"Time\"][window_size:] - df_final[\"Time\"][window_size]\n",
    "\n",
    "        time_years = np.array([(item / np.timedelta64(1, 'm')) / (60*24*365) for item in time_delta])\n",
    "\n",
    "        Y_TREND = y_pred_noscale_BI_GRU[y_pred_noscale_BI_GRU.astype('str') != 'nan']\n",
    "        X_TREND = time_years[y_pred_noscale_BI_GRU[:, 0].astype('str') != 'nan']\n",
    "\n",
    "        reg = LinearRegression().fit(X_TREND.reshape(-1,1), Y_TREND)\n",
    "\n",
    "        slope_BI_GRU = reg.coef_[0]\n",
    "        intercept_BI_GRU = reg.intercept_\n",
    "        \n",
    "        if final_train_loss_BI_GRU < tol:\n",
    "\n",
    "            print(N)\n",
    "\n",
    "            N += 1\n",
    "\n",
    "            slopes.append(slope_BI_GRU)\n",
    "            intercepts.append(intercept_BI_GRU)\n",
    "\n",
    "            train_errors.append(final_train_loss_BI_GRU)\n",
    "            val_errors.append(final_val_loss_BI_GRU)\n",
    "\n",
    "            N_epochs.append(epochs_used_BI_GRU)\n",
    "            training_time.append(time_elapsed_BI_GRU)\n",
    "            \n",
    "    return slopes, intercepts, train_errors, val_errors, N_epochs, training_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nf = 2\n",
    "\n",
    "number = 1\n",
    "\n",
    "type_ = \"BI_GRU\"\n",
    "\n",
    "if type_ == \"RNN\":\n",
    "\n",
    "    slopes, intercepts, train_errors, val_errors, N_epochs, training_time = RNN(Nf)\n",
    "    \n",
    "elif type_ == \"LSTM\":\n",
    "    \n",
    "    slopes, intercepts, train_errors, val_errors, N_epochs, training_time = LSTM_NN(Nf)\n",
    "    \n",
    "elif type_ == \"BI_LSTM\":\n",
    "    \n",
    "    slopes, intercepts, train_errors, val_errors, N_epochs, training_time = BI_LSTM_NN(Nf)\n",
    "    \n",
    "elif type_ == \"BI_GRU\":\n",
    "    \n",
    "    slopes, intercepts, train_errors, val_errors, N_epochs, training_time = BI_GRU_NN(Nf)\n",
    "    \n",
    "else:\n",
    "    \n",
    "    print(\"Undefined name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slopes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = \"Slope Intercept Train_error Val_error N_epochs Training_time\"\n",
    "\n",
    "np.savetxt(\"RNN_%s.txt\" % number, \n",
    "           np.transpose([slopes, intercepts, train_errors, val_errors, N_epochs, training_time]),\n",
    "           header=header)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
